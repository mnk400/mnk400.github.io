<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Manik Kumar</title>
    <description>Manik
</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Fri, 15 May 2020 20:12:02 -0400</pubDate>
    <lastBuildDate>Fri, 15 May 2020 20:12:02 -0400</lastBuildDate>
    <generator>Jekyll v3.6.3</generator>
    
      <item>
        <title>Maximum Likelihood Estimation of Gumbel Distribution</title>
        <description>&lt;h1 id=&quot;maximum-likelihood-estimation-of-gumbel-distribution&quot;&gt;Maximum Likelihood Estimation of Gumbel Distribution&lt;/h1&gt;

&lt;p&gt;This is a write up to derive the maximum liklihood solution for estimation of a gumbel distribution.&lt;/p&gt;

&lt;p&gt;Let’s start with the probability density function of the gumbel distribution, which is&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;p(x)=\frac{1}{\beta}e^{\frac{x-\alpha}{\beta}}e^{-e^{\frac{x-\alpha}{\beta}}}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\text{where }\alpha \text{ and } \beta \text{ are parameters and }\alpha \in \mathbb{R}, \beta &gt; 0&lt;/script&gt;

&lt;p&gt;Maximum liklihood solution can be written as,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathrm{f}_{M L}=\underset{\alpha \in \mathbb{R}, \beta&gt;0}{\operatorname{argmax}} \in \mathbb{R}, \beta&gt;0(P(D | \alpha, \beta))&lt;/script&gt;

&lt;p&gt;where &lt;em&gt;D&lt;/em&gt; represents the set of datapoints.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\mathrm{f}_{M L}=\underset{\alpha \in \mathbb{R}, \beta&gt;0}{\operatorname{argmax}} \in \mathbb{R}, \beta&gt;0(\prod_{i=1}^n P(x_i | \alpha, \beta))&lt;/script&gt;

&lt;p&gt;where &lt;em&gt;x&lt;/em&gt;&lt;sub&gt;&lt;em&gt;i&lt;/em&gt;&lt;/sub&gt; represents each datapoint.&lt;/p&gt;

&lt;p&gt;Now onto calculating the log-likelihood,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;=\ln \left(\prod_{i=0}^{n} P\left(x_{i} | \alpha, \beta\right)\right)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;=\sum_{i=0}^{n} \ln P\left(\left(x_{i} | \alpha, \beta\right)\right)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;=\sum_{i=0}^{n} \ln \left(\frac{1}{\beta} e^{-\frac{y_{i}-\alpha}{\beta}} e^{-e^{-\frac{j_{j}-a}{\beta}}}\right)&lt;/script&gt;

&lt;p&gt;We end up with an equation with 2 variables in it, we’ll be using &lt;a href=&quot;http://www.sosmath.com/calculus/diff/der07/der07.html&quot;&gt;the Newton-Raphson method&lt;/a&gt; to approximate the roots of the equation. I recommend reading through the link if you’re familiar with the approximation method.&lt;/p&gt;

&lt;p&gt;Finding derivatives, with respect to both &lt;em&gt;α&lt;/em&gt; and &lt;em&gt;β&lt;/em&gt;.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial f}{\partial \beta}=\sum_{i=0}^{n} \frac{x_{i}-n}{\beta^{2}}-\frac{n}{\beta}-\sum_{i=0}^{n} \frac{x_{i}-\alpha}{\beta} e^{-\frac{z_{i}-\alpha}{\beta}}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial f}{\partial \alpha}=\frac{n}{\beta}-\frac{1}{\beta} \sum_{i=0}^{n} e^{-\frac{z_{i}-\alpha}{\beta}}&lt;/script&gt;

&lt;p&gt;For the Newron-Raphson approximation method, we require double derivatives, which we use to calculate the Hessian matrix.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial^{2} f}{\partial \beta^{2}}=\frac{n}{\beta^{2}}-\frac{2}{\beta^{2}} \sum_{i=0}^{n}\left(x_{i}-\alpha\right)+\frac{2}{\beta^{3}} \sum_{i=0}^{n}\left(x_{i}-\alpha\right) e^{\frac{-\left(x_{i}-\alpha\right)}{\beta}}+\frac{2}{\beta^{4}} \sum_{i=0}^{n}\left(x_{i}-\alpha\right)^{2} e^{\frac{-\left(x_{j}-\alpha\right)}{\beta}}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial^{2} f}{\partial \alpha^{2}}=\frac{-i}{\beta^{2}} \sum_{i=0}^{n} e^{-\frac{x_{i}-a}{\beta}}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial^{2} f}{\partial \alpha \beta}=-\frac{n}{\beta^{2}}+\frac{1}{\beta^{2}} \sum_{i=0}^{n} e^{\frac{-\left(x_{i}-\alpha\right)}{\beta}}-\frac{1}{\beta^{3}} \sum_{i=0}^{n}\left(x_{i}-\alpha\right) e^{\frac{-\left(x_{i}-a\right)}{\beta}}&lt;/script&gt;

&lt;p&gt;The Hessian matrix can be calculated by,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
H=\left[\begin{array}{ll}\frac{\partial^{2} f}{\partial \alpha^{2}} &amp; \frac{\partial^{2} f}{\partial \alpha \beta} \\ \frac{\partial^{2} f}{\partial \alpha \beta} &amp; \frac{\partial^{2} f}{\partial \beta^{2}}\end{array}\right] %]]&gt;&lt;/script&gt;

&lt;p&gt;We also need a f matrix with the equations we’re solving for,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f=\left[\begin{array}{l}\frac{\partial f}{\partial \alpha} \\ \frac{\partial f}{\partial \beta}\end{array}\right]&lt;/script&gt;

&lt;p&gt;Now we can follow the following algorithm to estimate the parameters.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 1 :&lt;/strong&gt;
We’ll choose starting values for α and β, α(0) and β(0) using the method of moment estimators.&lt;/p&gt;

&lt;p&gt;β = 0.7977∗ standard deviation of the dataset
α = mean of the dataset - 0.4501 * standard deviation&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 2 :&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Choose a tolerance t for the change, 10^(−10) in our case. If the level of changes are less than the tolerance the iterations will break.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 3 :&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Obtain inverse of the Hessian Matrix inv(H(α(0))(β(0))) and f(α(0))(β(0))&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 4 :&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Obtain new values of α and β, α(new) and β(new), from the Newton Raphson algorithm,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\left[\begin{array}{l}\alpha^{(n e w)} \\ \beta^{(n e w)}\end{array}\right]=\left[\begin{array}{l}\alpha^{(o l d)} \\ \beta^{(o l d)}\end{array}\right]-H^{-1}\left(\alpha^{(o l d)}, \beta^{(o l d)}\right) f\left(\alpha^{(o l d)}, \beta^{(o l d)}\right)&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Step 5 :&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Check to see if the differences between new and the old values are small enough and compare it by the set tolerance,&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\left[\left(\alpha^{(n e w)}-\alpha^{(o l d)}\right)^{2}+\left(\beta^{(n e w)}-\beta^{(o l d)}\right)^{2}&lt;t\right] %]]&gt;&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;if No, then return to Step(3), and calculate H−1(α(new))(β(new)) and f(α(new))(β(new)) and keep iterating.&lt;/li&gt;
  &lt;li&gt;if Yes, then stop.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Following are the results of the above algorithm for a gumbel destibution generated using α = 2.3 and β = 4.0 ran multiple times and averaged.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://i.imgur.com/EjvbGg5.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/mnk400/gumbelMLE&quot;&gt;Link to the code.&lt;/a&gt;&lt;/p&gt;
</description>
        <pubDate>Thu, 06 Feb 2020 00:00:00 -0500</pubDate>
        <link>http://localhost:4000/2020/02/gumble-mle.html</link>
        <guid isPermaLink="true">http://localhost:4000/2020/02/gumble-mle.html</guid>
        
        <category>Machine</category>
        
        <category>Learning,</category>
        
        <category>Estimation</category>
        
        
      </item>
    
  </channel>
</rss>

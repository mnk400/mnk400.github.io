<!DOCTYPE html>
<html>

    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">

        <script async src="https://www.googletagmanager.com/gtag/js?id=G-WLJ3VCYPD9"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-WLJ3VCYPD9');
</script>
        
        <title>Maximum Likelihood Estimation of Gumbel Distribution — Manik</title>
        

        <meta name="description" content="My Portfolio and Blog.">

        <link rel="icon" href="/assets/favicon.ico">
        <link rel="stylesheet" type="text/css" href="/assets/css/main.css">
        <link href="https://fonts.googleapis.com/css2?family=IBM+Plex+Mono&display=swap" rel="stylesheet">
    </head>

    <body>

        <div class="wrapper">
            <script type="text/x-mathjax-config">
	MathJax.Hub.Config({
		"HTML-CSS" : {
			availableFonts : ["Tex"],
			scale : 70
		}
	});
</script>

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript">

</script>

<div class="post">
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-WLJ3VCYPD9"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-WLJ3VCYPD9');
</script>
	<a class="post__back" href="/">←</a>

	
		<h1 class="post__title">Maximum Likelihood Estimation of Gumbel Distribution</h1>
	

	
		<p class="post__date">February 6, 2020.
	

	
	Edited on May 14, 2020</p>
	

    <div class="post__content"?>
        <p><span class="note">This write up assumes basic knowledge of the estimation theory.</span></p>

<p>Let’s start with the probability density function of the gumbel distribution, which is</p>

\[p(x)=\frac{1}{\beta}e^{\frac{x-\alpha}{\beta}}e^{-e^{\frac{x-\alpha}{\beta}}}\]

\[\text{where }\alpha \text{ and } \beta \text{ are parameters and }\alpha \in \mathbb{R}, \beta &gt; 0\]

<p>Maximum liklihood solution can be written as,</p>

\[\mathrm{f}_{M L}=\underset{\alpha \in \mathbb{R}, \beta&gt;0}{\operatorname{argmax}} \in \mathbb{R}, \beta&gt;0(P(D | \alpha, \beta))\]

<p>where <em>D</em> represents the set of datapoints.</p>

\[\mathrm{f}_{M L}=\underset{\alpha \in \mathbb{R}, \beta&gt;0}{\operatorname{argmax}} \in \mathbb{R}, \beta&gt;0(\prod_{i=1}^n P(x_i | \alpha, \beta))\]

<p>where <em>x</em><sub><em>i</em></sub> represents each datapoint.</p>

<p>Now onto calculating the log-likelihood,</p>

\[=\ln \left(\prod_{i=0}^{n} P\left(x_{i} | \alpha, \beta\right)\right)\]

\[=\sum_{i=0}^{n} \ln P\left(\left(x_{i} | \alpha, \beta\right)\right)\]

\[=\sum_{i=0}^{n} \ln \left(\frac{1}{\beta} e^{-\frac{y_{i}-\alpha}{\beta}} e^{-e^{-\frac{j_{j}-a}{\beta}}}\right)\]

<p>We end up with an equation with 2 variables in it, we’ll be using <a href="http://www.sosmath.com/calculus/diff/der07/der07.html">the Newton-Raphson method</a> to approximate the roots of the equation. I recommend reading through the link if you’re familiar with the approximation method.</p>

<p>Finding derivatives, with respect to both <em>α</em> and <em>β</em>.</p>

\[\frac{\partial f}{\partial \beta}=\sum_{i=0}^{n} \frac{x_{i}-n}{\beta^{2}}-\frac{n}{\beta}-\sum_{i=0}^{n} \frac{x_{i}-\alpha}{\beta} e^{-\frac{z_{i}-\alpha}{\beta}}\]

\[\frac{\partial f}{\partial \alpha}=\frac{n}{\beta}-\frac{1}{\beta} \sum_{i=0}^{n} e^{-\frac{z_{i}-\alpha}{\beta}}\]

<p>For the Newron-Raphson approximation method, we require double derivatives, which we use to calculate the Hessian matrix.</p>

\[\frac{\partial^{2} f}{\partial \beta^{2}}=\frac{n}{\beta^{2}}-\frac{2}{\beta^{2}} \sum_{i=0}^{n}\left(x_{i}-\alpha\right)+\frac{2}{\beta^{3}} \sum_{i=0}^{n}\left(x_{i}-\alpha\right) e^{\frac{-\left(x_{i}-\alpha\right)}{\beta}}+\frac{2}{\beta^{4}} \sum_{i=0}^{n}\left(x_{i}-\alpha\right)^{2} e^{\frac{-\left(x_{j}-\alpha\right)}{\beta}}\]

\[\frac{\partial^{2} f}{\partial \alpha^{2}}=\frac{-i}{\beta^{2}} \sum_{i=0}^{n} e^{-\frac{x_{i}-a}{\beta}}\]

\[\frac{\partial^{2} f}{\partial \alpha \beta}=-\frac{n}{\beta^{2}}+\frac{1}{\beta^{2}} \sum_{i=0}^{n} e^{\frac{-\left(x_{i}-\alpha\right)}{\beta}}-\frac{1}{\beta^{3}} \sum_{i=0}^{n}\left(x_{i}-\alpha\right) e^{\frac{-\left(x_{i}-a\right)}{\beta}}\]

<p>The Hessian matrix can be calculated by,</p>

\[H=\left[\begin{array}{ll}\frac{\partial^{2} f}{\partial \alpha^{2}} &amp; \frac{\partial^{2} f}{\partial \alpha \beta} \\ \frac{\partial^{2} f}{\partial \alpha \beta} &amp; \frac{\partial^{2} f}{\partial \beta^{2}}\end{array}\right]\]

<p>We also need a f matrix with the equations we’re solving for,</p>

\[f=\left[\begin{array}{l}\frac{\partial f}{\partial \alpha} \\ \frac{\partial f}{\partial \beta}\end{array}\right]\]

<p>Now we can follow the following algorithm to estimate the parameters.</p>

<p><strong>Step 1 :</strong>
We’ll choose starting values for α and β, α(0) and β(0) using the method of moment estimators.</p>

<p>β = 0.7977∗ standard deviation of the dataset
α = mean of the dataset - 0.4501 * standard deviation</p>

<p><strong>Step 2 :</strong></p>

<p>Choose a tolerance t for the change, 10^(−10) in our case. If the level of changes are less than the tolerance the iterations will break.</p>

<p><strong>Step 3 :</strong></p>

<p>Obtain inverse of the Hessian Matrix inv(H(α(0))(β(0))) and f(α(0))(β(0))</p>

<p><strong>Step 4 :</strong></p>

<p>Obtain new values of α and β, α(new) and β(new), from the Newton Raphson algorithm,</p>

\[\left[\begin{array}{l}\alpha^{(n e w)} \\ \beta^{(n e w)}\end{array}\right]=\left[\begin{array}{l}\alpha^{(o l d)} \\ \beta^{(o l d)}\end{array}\right]-H^{-1}\left(\alpha^{(o l d)}, \beta^{(o l d)}\right) f\left(\alpha^{(o l d)}, \beta^{(o l d)}\right)\]

<p><strong>Step 5 :</strong></p>

<p>Check to see if the differences between new and the old values are small enough and compare it by the set tolerance,</p>

\[\left[\left(\alpha^{(n e w)}-\alpha^{(o l d)}\right)^{2}+\left(\beta^{(n e w)}-\beta^{(o l d)}\right)^{2}&lt;t\right]\]

<ul>
  <li>if No, then return to Step(3), and calculate H−1(α(new))(β(new)) and f(α(new))(β(new)) and keep iterating.</li>
  <li>if Yes, then stop.</li>
</ul>

<p>Following are the results of the above algorithm for a gumbel destibution generated using α = 2.3 and β = 4.0 ran multiple times and averaged.</p>

<p><img src="https://i.imgur.com/EjvbGg5.png" alt="" /></p>

<p><a href="https://github.com/mnk400/gumbelMLE">Link to the code.</a></p>

    </div>
</div>

        </div>

    </body>

</html>
